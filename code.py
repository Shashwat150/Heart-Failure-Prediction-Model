# -*- coding: utf-8 -*-
"""PRML_COURSE_PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ui5qDG5KLaLL21dUyC6LqI697zj6Cv0Y

Data = https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction

TEAM MEMBERS - Nishant Sharma(B20EE039) , Neielotpal Rao(B20EE038) and 
Shashwat Singh(B20CS066)
"""

import pandas as pd
import numpy as np
import csv
import matplotlib.pyplot as plt
import pandas as pd
import sklearn
import seaborn as sns
from sklearn import metrics
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import accuracy_score , classification_report, f1_score, recall_score, precision_score

data = pd.read_csv("/content/heart (1).csv")

data.head()

data.isna().sum()

data.describe()

sns.pairplot(data,hue='HeartDisease')

plt.figure(figsize=(10,7))
sns.heatmap(data.corr(), annot=True,cmap='viridis')
plt.show()

plt.figure(figsize=(8, 8))
labels=['Female', 'Male']
plt.pie(data['Sex'].value_counts().sort_values(),autopct='%1.2f%%',labels=labels,
        colors=['lightblue','royalblue'])
plt.title('Gender analysis')
plt.axis('equal')
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(x=data['HeartDisease'],hue=data['ChestPainType'])
plt.xticks(np.arange(2), ['0', '1']) 
plt.show()

plt.figure(figsize=(10,5))
sns.countplot(x=data.HeartDisease,hue=data.ExerciseAngina)
plt.xticks(np.arange(2), ['0', '1']) 
plt.show()

plt.figure(figsize=(15,7))
plt.title('Age vs Heart Disease')

sns.histplot(data=data, x='Age', hue='HeartDisease', bins=30, kde=True);

def encoder(new,a):

  from sklearn.preprocessing import LabelEncoder
  le = LabelEncoder()
  for i in a:
    new[i] = le.fit_transform(new[i])

  return

data.head()

data['FastingBS'].value_counts()

#
data_encoded = pd.get_dummies(data)

data_encoded.head()

X = data_encoded.drop(['HeartDisease'], axis=1)
y = data_encoded['HeartDisease']

scaler  =  StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

# X_train,X_test,y_train,y_test = train_test_split(X , y, test_size =0.3 , random_state=42)
X_train,X_test,y_train,y_test = train_test_split(X_scaled , y, test_size =0.3 , random_state=42)

def model(model, X, y, X_, y_):
  model.fit(X,y)
  y_p = model.predict(X_)
  print(model, 'Accuracy :',accuracy_score(y_,y_p),'\n','\t\t\t','f1_score :',f1_score(y_,y_p),'\n','\t\t\t','recall_score :',recall_score(y_,y_p),'\n','\t\t\t','precision_score :',precision_score(y_,y_p))
  return

from sklearn.tree import DecisionTreeClassifier
model(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)

dt = DecisionTreeClassifier().fit(X_train, y_train)
ROC_DT = RocCurveDisplay.from_estimator(dt, X_test, y_test)
plt.show()

from sklearn.svm import SVC
model(SVC(kernel='rbf',C=1, gamma = 0.001), X_train, y_train, X_test, y_test)

from xgboost import XGBClassifier
model(XGBClassifier(), X_train, y_train, X_test, y_test)

xgb = XGBClassifier().fit(X_train, y_train)
ROC_XGB = RocCurveDisplay.from_estimator(xgb, X_test, y_test)
plt.show()

from lightgbm import LGBMClassifier
model(LGBMClassifier(), X_train, y_train, X_test, y_test)

from sklearn.ensemble import RandomForestClassifier

y_pred_rfc = model(RandomForestClassifier(), X_train, y_train, X_test, y_test)

from sklearn.neighbors import KNeighborsClassifier
model(KNeighborsClassifier(n_neighbors=30),X_train, y_train, X_test, y_test)

#Deep neural Network
from keras.callbacks import ModelCheckpoint
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense

NN_model = Sequential()


NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))

NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))


NN_model.add(Dense(1, kernel_initializer='normal',activation='sigmoid'))

NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

NN_model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)

NN_y_pred = NN_model.predict(X_test)

Y_nn = []

for i in range(len(NN_y_pred)):
  if (NN_y_pred[i]>0.5):
    Y_nn.append(1)
  else:
    Y_nn.append(0)

print(NN_model, 'Accuracy :',accuracy_score(y_test,Y_nn),'\n','\t\t\t','f1_score :',f1_score(y_test,Y_nn),'\n','\t\t\t','recall_score :',recall_score(y_test,Y_nn),'\n','\t\t\t','precision_score :',precision_score(y_test,Y_nn))

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=2, random_state=43,init='random',
     max_iter=500)

y_km = kmeans.fit_predict(X_train)

plt.figure(figsize=(10,5))
data = np.array(X_train)
plt.scatter(
    data[y_km == 0, 0], data[y_km == 0, 1],
    s=50, c='lightgreen',
    marker='s', edgecolor='black',
    label='cluster 1'
)

plt.scatter(
    data[y_km == 1, 0], data[y_km == 1, 1],
    s=50, c='orange',
    marker='o', edgecolor='black',
    label='cluster 2'
)

plt.scatter(
    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
    s=150, marker='^',
    c='red', edgecolor='black',
    label='centroids'
)
plt.legend(scatterpoints=1)
plt.grid()
plt.show()

distortions = []
for i in range(1, 11):
    km = KMeans(
        n_clusters=i, init='random',
        n_init=10, max_iter=300,
        tol=1e-04, random_state=0
    )
    km.fit(X_train)
    distortions.append(km.inertia_)

# plot
plt.figure(figsize=(10,6))
plt.plot(range(1, 11), np.log(distortions), marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.title('Elbow Method')
plt.grid()
plt.show()

"""MODEL COMPARISON"""

#Using Cross Validation Scores
def cross_val(model,X,y) :
  from sklearn.model_selection import cross_val_score

  scores = cross_val_score(model, X, y, cv=5)
  plt.figure(figsize=(10,6))
  plt.plot(range(1,6), scores)
  plt.title(model)
  plt.xlabel('nth interation')
  plt.ylabel('score')
  plt.ylim(0.75, 0.95)
  plt.grid()
  plt.show()
  print(model,'Cross Validation Score',scores)
  return

M = (SVC(), RandomForestClassifier(), LGBMClassifier(), KNeighborsClassifier())
for i in M:
  cross_val(i,X_train,y_train);
  print('\n')

"""Now Tune the Hyperparameter of best models using Grid aur RandomizedSearchCV"""

from sklearn.model_selection import RandomizedSearchCV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]

# Number of features to consider at every split
max_features = ['auto', 'sqrt']

# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]

# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]

# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]

# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}

# Random search of parameters, using 5 fold cross validation, 
# search across 100 different combinations
rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid,scoring='f1', n_iter = 20, cv = 5, verbose=-1, random_state=42)

rf_random.fit(X_train,y_train)

rf_random.best_params_

model(RandomForestClassifier(max_depth = 20,
                             max_features = 'auto',
                             min_samples_leaf = 10,
                             min_samples_split = 5,
                             n_estimators = 700), X_train, y_train, X_test, y_test)

from sklearn.model_selection import GridSearchCV

parameters =  {'C': [1, 10, 100, 1000], 'kernel': ['linear','rbf'],
               'gamma' : [0.0001,0.001,0.01,0.1,1]}
 

svc_s = GridSearchCV(estimator = SVC(), param_grid = parameters ,scoring='f1', cv = 7, verbose=-1)

svc_s.fit(X_train,y_train)

svc_s.best_params_

from sklearn.svm import SVC
model(SVC(kernel='rbf',C=10, gamma = 0.01), X_train, y_train, X_test, y_test)

from sklearn.model_selection import GridSearchCV

grid_params = { 'n_neighbors' : [5,7,10,15,20,30],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan'],
               'p' : [1,2]}



knn_2 = KNeighborsClassifier()

clf = GridSearchCV(knn_2,grid_params, cv=5)


best_model = clf.fit(X_train,y_train)


best_model.best_params_

model(KNeighborsClassifier(metric= 'minkowski',n_neighbors= 30, p= 1,weights= 'distance'), X_train, y_train, X_test, y_test)

from sklearn.model_selection import RandomizedSearchCV

param_grid_lgm = {
   'n_estimators': [100,500,1000,1500] , 
    'learning_rate': [0.001,0.01,0.1,0.5,1],
    'colsample_bytree': [0.6,0.8,1],
    "max_depth": [5,10,50,100,500]
}

Random_search = RandomizedSearchCV(estimator = LGBMClassifier(), param_distributions = param_grid_lgm,scoring='f1', n_iter = 20, cv = 5,verbose=-1)

Random_search.fit(X_train,y_train)

Random_search.best_params_

from lightgbm import LGBMClassifier

lgm = LGBMClassifier(colsample_bytree= 0.6,learning_rate= 0.01,max_depth= 50,n_estimators= 500).fit(X_train, y_train)
ROC_LGM = RocCurveDisplay.from_estimator(lgm, X_test, y_test)
plt.show()

from sklearn.metrics import RocCurveDisplay
 
md = RandomForestClassifier(max_depth= 20,max_features= 'auto',min_samples_leaf= 10,min_samples_split= 5,n_estimators= 700).fit(X_train, y_train)

ROC_RF = RocCurveDisplay.from_estimator(md, X_test, y_test)
plt.show()

mvc = SVC(kernel='rbf',C=1, gamma = 0.001,probability=True).fit(X_train, y_train)

ROC_SVC = RocCurveDisplay.from_estimator(mvc, X_test, y_test)
plt.show()

model(KNeighborsClassifier(n_neighbors = 30, weights = 'distance',algorithm = 'brute',metric = 'minkowski'),X_train,y_train,X_test,y_test)

knn = KNeighborsClassifier(n_neighbors = 30, weights = 'distance',algorithm = 'brute',metric = 'minkowski').fit(X_train, y_train)

ROC_KNN = RocCurveDisplay.from_estimator(knn, X_test, y_test)
plt.show()

"""ROC CURVE COMPARISON"""

import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve

pred_prob1 = md.predict_proba(X_test)
pred_prob2 = mvc.predict_proba(X_test)

fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)

 
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)
plt.style.use('seaborn')


plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Random Forest Classifier')
plt.plot(fpr2, tpr2, linestyle='--',color='green', label='SVC')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')

plt.title('ROC curve')

plt.xlabel('False Positive Rate')

plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve

pred_prob1 = md.predict_proba(X_test)
pred_prob2 = lgm.predict_proba(X_test)

fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)

 
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)
plt.style.use('seaborn')


plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Random Forest Classifier')
plt.plot(fpr2, tpr2, linestyle='--',color='green', label='Light GBM')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')

plt.title('ROC curve')

plt.xlabel('False Positive Rate')

plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve

pred_prob1 = md.predict_proba(X_test)
pred_prob2 = knn.predict_proba(X_test)

fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)

 
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)
plt.style.use('seaborn')


plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Random Forest Classifier')
plt.plot(fpr2, tpr2, linestyle='--',color='green', label='K-nearest Neighbour')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')

plt.title('ROC curve')

plt.xlabel('False Positive Rate')

plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve

pred_prob1 = lgm.predict_proba(X_test)
pred_prob2 = knn.predict_proba(X_test)

fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)

 
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)
plt.style.use('seaborn')


plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Light GBM')
plt.plot(fpr2, tpr2, linestyle='--',color='green', label='K-nearest Neighbour')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')

plt.title('ROC curve')

plt.xlabel('False Positive Rate')

plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();

#As we can obeserve from these plots that KNN and Random forest are the best algorithms for our task
#################

import pickle
KNN = KNeighborsClassifier(metric= 'minkowski',n_neighbors= 30, p= 1,weights= 'distance')
KNN.fit(X,y)

#Finally Saving the model for Future USe

model_filename = 'finalized_model_KNN.sav'
saved_model = pickle.dump(KNN, open(model_filename,'wb'))

#Now we can use it for implementing a WebApp or any medical devices